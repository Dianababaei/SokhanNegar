# Whisper API Integration Testing - Quick Reference

## Test Files Summary

| File | Purpose | Tests | Duration |
|------|---------|-------|----------|
| `test_whisper_integration.py` | Core integration tests | 33 | 2-3 min |
| `test_network_simulation.py` | Network/fallback tests | 6 | 1-2 min |
| `test_quality_comparison.py` | Quality framework | 3 | 5-10 min |
| `test_performance_monitor.py` | Stability tests | 1 | 10-15 min |
| `test_audio_helper.py` | Sample generation | 6 | <1 min |

**Total**: 49+ tests, ~20-30 minutes

---

## One-Command Test Execution

```bash
# Generate samples first
python test_audio_helper.py

# Run all major test suites
python test_whisper_integration.py && \
python test_network_simulation.py && \
python test_quality_comparison.py && \
python test_performance_monitor.py
```

---

## Individual Test Modules

### 1. Audio Sample Generation
```bash
python test_audio_helper.py
```
**Output**: 6 WAV files in `test_samples/`
**Purpose**: Setup for all other tests

---

### 2. Core Integration Tests (33 tests)
```bash
python test_whisper_integration.py
```

**Test Categories**:
- API Key Validation (4 tests)
  - Valid key, invalid key, missing key, malformed key
- Network Handling (4 tests)
  - Connection errors, timeouts, DNS, socket errors
- Fallback Behavior (4 tests)
  - Auth error fallback, rate limit, API errors
- Usage Tracking (9 tests)
  - Duration accuracy, cost calculation, persistence, recovery
- Audio Format (3 tests)
  - WAV conversion, invalid data, structure validation
- Continuous Operation (3 tests)
  - Multiple chunks, memory stability, thread safety
- Edge Cases (5 tests)
  - Empty audio, short audio, rate limits, invalid durations
- Integration (1 test)
  - Full workflow with tracking

**Expected**: All 33 tests pass âœ“

---

### 3. Network & Fallback Tests
```bash
python test_network_simulation.py
```

**Tests**:
1. âœ“ AuthenticationError â†’ Fallback
2. âœ“ RateLimitError â†’ Fallback
3. âœ“ ConnectionError â†’ Fallback
4. âœ“ Timeout (30s) â†’ Fallback
5. âœ“ DNS Error â†’ Fallback
6. âœ“ Service Status Update (ðŸŸ¢ â†’ ðŸŸ¡)

**Expected**: 6/6 passed

---

### 4. Quality Comparison
```bash
python test_quality_comparison.py
```

**Outputs**:
- `quality_test_template.json` - Manual testing template
- `quality_comparison_report.json` - Comparison results

**For Real Testing**:
1. Record Persian speech samples
2. Transcribe with both Whisper and Google APIs
3. Fill in `quality_test_template.json`
4. Re-run comparison

---

### 5. Performance & Stability
```bash
python test_performance_monitor.py
```

**Metrics Monitored**:
- Memory: Peak, min, average
- CPU: Peak, average
- Processing: Consistency, chunks/second
- Duration: 10 minutes simulated

**Output**: `continuous_operation_results.json`

**Success Criteria**:
- Memory stable (no >10% leak)
- CPU average <50%
- No crashes
- Consistent processing

---

## Key Test Specifications

### API Key Tests
```
âœ“ Valid key â†’ Whisper succeeds
âœ“ Invalid key â†’ Google fallback
âœ“ Missing key â†’ Error + fallback
âœ“ Malformed key â†’ Caught gracefully
```

### Network Tests
```
âœ“ Normal operation â†’ Works
âœ“ Connection error â†’ Fallback triggered
âœ“ Timeout (30s) â†’ Graceful handling
âœ“ DNS error â†’ Fallback initiated
```

### Fallback Tests
```
âœ“ Whisper fails â†’ Google processes request
âœ“ Smooth transition â†’ No data loss
âœ“ UI updates â†’ ðŸŸ¢ â†’ ðŸŸ¡ indicator
```

### Usage Tracking Tests
```
âœ“ Duration accuracy: Â±5% tolerance
âœ“ Cost calculation: minutes Ã— $0.006
âœ“ Persistence: Data survives restarts
âœ“ Corrupted recovery: Auto-repair
âœ“ Daily/weekly aggregates: Tracked
```

### Performance Tests
```
âœ“ Memory: Peak < baseline + 10%
âœ“ CPU: Average <50%, Peak <80%
âœ“ Processing: Consistent Â±20%
âœ“ Stability: No crashes/hangs
```

### UI Tests
```
âœ“ Service indicator updates correctly
âœ“ Usage stats display accurately
âœ“ UI stays responsive
âœ“ Persian text renders correctly
```

---

## Test Results Checklist

After running all tests:

- [ ] `test_whisper_integration.py`: 33 tests passed
- [ ] `test_network_simulation.py`: 6 tests passed
- [ ] `test_quality_comparison.py`: Report generated
- [ ] `test_performance_monitor.py`: Results saved
- [ ] `test_audio_helper.py`: Samples generated
- [ ] No unhandled exceptions
- [ ] All error cases handled gracefully
- [ ] Logs reviewed for issues
- [ ] Metrics within acceptable ranges

---

## Quick Troubleshooting

| Issue | Solution |
|-------|----------|
| "OPENAI_API_KEY not found" | Set up .env file with your API key |
| "No module named X" | Run `pip install -r requirements.txt` |
| "psutil not available" | Run `pip install psutil` (optional) |
| "Audio file not found" | Run `python test_audio_helper.py` first |
| Tests timeout | Check internet connection and API limits |
| Permission errors | Verify ~/.sokhan_negar directory exists |

---

## Output Files Generated

After running all tests:

```
project_root/
â”œâ”€â”€ transcription.log              (Application logs)
â”œâ”€â”€ test_samples/                  (Generated audio samples)
â”œâ”€â”€ test_results/
â”‚   â”œâ”€â”€ quality_comparison_report.json
â”‚   â””â”€â”€ continuous_operation_results.json
â”œâ”€â”€ quality_test_template.json     (For manual quality testing)
â””â”€â”€ .sokhan_negar/
    â””â”€â”€ usage_data.json            (Usage tracking data)
```

---

## Success Criteria Summary

| Category | Criterion | Status |
|----------|-----------|--------|
| API Keys | Valid/invalid/missing handling | âœ… Tested |
| Network | Error handling and fallback | âœ… Tested |
| Fallback | Smooth transition, UI updates | âœ… Tested |
| Quality | Comparison framework ready | âœ… Ready |
| Usage | Accuracy Â±5%, persistence | âœ… Tested |
| Performance | No leaks, stable CPU | âœ… Tested |
| UI | Responsive, correct display | âœ… Defined |
| Edge Cases | Error handling | âœ… Tested |

---

## Next Steps

1. **Run All Tests** â†’ `python test_audio_helper.py && python test_whisper_integration.py && ...`
2. **Review Results** â†’ Check all output files and logs
3. **Quality Testing** â†’ Use `quality_test_template.json` with real speech samples
4. **Fix Issues** â†’ Address any failures
5. **Document** â†’ Update test_report.md with results
6. **Deploy** â†’ When all tests pass

---

## Documentation Files

- `test_report.md` - Detailed test specifications (33 tests)
- `TEST_EXECUTION_GUIDE.md` - Complete execution instructions
- `TESTING_QUICK_REFERENCE.md` - This file
- `test_samples/README.md` - Audio sample information

---

## Key Files for Reference

**Main Application**:
- `SokhanNegar.py` - Main UI and orchestration
- `whisper_api.py` - Whisper API integration
- `usage_tracker.py` - Usage tracking module
- `config.py` - Configuration management

**Test Files**:
- `test_whisper_integration.py` - 33 core tests
- `test_network_simulation.py` - Network simulation
- `test_quality_comparison.py` - Quality comparison
- `test_performance_monitor.py` - Performance monitoring
- `test_audio_helper.py` - Audio sample generation

---

**Ready to Execute**: All test infrastructure is in place. See TEST_EXECUTION_GUIDE.md for detailed instructions.
